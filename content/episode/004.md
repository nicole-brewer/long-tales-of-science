---
Description: In this episode, Nicole interviews Elizabeth Aslinger, who recently defended her dissertation for which she used mathematical modeling to identify people who have schizophrenia. We talk about her experience managing software and students in a computational psychology lab, teaching herself standards of practice, and what her priorities as she is choosing the next step in her career.
aliases:
- /3
author: Nicole
date: "2022-05-18"
episode: "4"
episode_image: img/episode/004.png
explicit: "false"
guests:
- easlinger
hosts:
- nbrewer
images:
- http://www.google.comimg/episode/default-social.jpg
tags: ['computational psychology', 'testing', 'user interface', 'graduate student', 'management']
podcast_bytes: "27814019"
podcast_duration: "0:24:00"
podcast_file: 004.mp3
title: Trial by Fire 
#youtube: 
---
[00:00:32] **Nicole:** Elizabeth Alsinger completed her PhD in mathematical and computational psychology and M.S. In clinical psychology at Purdue university. As an undergraduate, she studied neurobiology and philosophy of science at Yale university. Ultimately graduating with a B a in a interdisciplinary track of political science.

[00:00:53] After college, she worked as a learning center teacher and led psychoeducational groups in a psychiatric hospital. While co-authoring a chapter about neuroscience methodology with Scott Lilienfeld at Emory, she later conducted research in personality pathology as a fellow in Aiden Wright's lab at the university of Pittsburgh. At Purdue she worked as an assistant clinician and the lab component of a PhD level statistics course, which dovetailed with her research interests in psychiatric classification and statistical methodology. Her substantive projects have involved investigations of affective and interpersonal processes, narcissism, externalizing symptoms, PTSD, suicidality, cognitive neuroscience and psychosis. Her broader research program revolves around developing mathematical models and computational tools, EG scientific software or application in scientific research broadly and in psychology and neuroscience in particular.

[00:02:11] So a very warm welcome to the show. Elizabeth, it's very good to have. 

[00:02:17] **Elizabeth:** Thank you for having me. 

[00:02:20] **Nicole:** So, first of all, congratulations on defending your dissertation. 

[00:02:26] **Elizabeth:** Thank you. 

[00:02:27] **Nicole:** And this interview is a bit unique because I have actually worked with Elizabeth in the past on a scientific software project. She was a lead graduate research assistant, and I was on the project as an RSE. But before we get started in all that, I want to know more about your background. I actually had no idea that you had such an interesting path.

[00:02:54] **Elizabeth:** Charitable way to put it. 

[00:02:56] **Nicole:** No, it is interesting. So why don't you go ahead and tell us how you got here. 

[00:03:01] **Elizabeth:** Yeah. I suppose I have pretty broad interests. And the funny thing actually was when I was younger you know, 13, 14 , there are a lot of different careers I can pursue that I'd be interested in neuro-linguistics international relations, but the one thing I'll never be as a quantitative psychologist.

[00:03:20] And then I became a quantitative psychologist and I'm like, well, okay, that's the one I'm interested in, tons of things in psychology and neuroscience, philosophy of science, methodology, stuff like that. The one thing I'll never really get into is cognitive psychology. And then that became one of my major focus.

[00:03:38] But yeah, I think it, it kind of started out with a strong interest substantively in psychology. And when I got to college, I was a part of this debate society, not like high school debate more of more of like a literary society or, you know, an intellectual group.

[00:03:55] And I got very interested in philosophy. And so even though I was a molecular biology major at the time focusing on neuroscience I started taking a lot of history courses, philosophy courses, et cetera. And so the second week of my senior year, I looked at the classes I wanted to take and looked at which majors that could fulfill and turned out this like super interdisciplinary track of political science would let me graduate.

[00:04:19] I, I had not taken a political science class until the last year of college, but able to complete the major was very flexible in that way. But after, after that I realized how much I really missed doing science rather than just talking about how science is done and how that affects society. And , what kind of inferences people are making or not making.

[00:04:43] So, I started volunteering in Scott Lilienfeld lab and working in the psychiatric hospitals and I started to kind of those two interests in, or those three interests, I suppose, in neuroscience and philosophy and psychology. Those streams kind of merged together. And I think that the natural output of that was of focus on measurement and assessment and developing actual tools, both, you know, in terms of methodology and math and statistics, as well as software.

[00:05:13] **Nicole:** Yeah. So this is really interesting. We have similar backgrounds. I'm also was an interdisciplinary major that I desperately just tried to string together a whole bunch of classes that I had taken. And I was also really interested in science and ended up in computing. Just because I saw that, I could be kind of a generalist, right.

[00:05:38] Like I didn't have to deep dive too hard into any, any one topic. 

[00:05:42] **Elizabeth:** Yeah, it's fun to be a generalist. You get to do lots of different types of things. It is, it can be a little tough in academia because you know the interdisciplinary stuff is not really well rewarded unless you leverage interdisciplinary methods to study, you know, a very narrow range of issues.

[00:06:02] So that's definitely been a little bit difficult to navigate, but I'd prefer to kind of focus the research I want and then figure out how to make that work later. 

[00:06:12] **Nicole:** Sure. Yeah. So like I said, I know Elizabeth through a project we were both on, related to power analysis of study design. Elizabeth worked on the backend computational software. And i worked on the graphical user interface. So why don't you go ahead and tell everyone about what that project is and your role in it. 

[00:06:41] **Elizabeth:** Right. So we are designing software that essentially. Researchers figure out how to design their experiments, in particular, how many people to recruit in order to have the, have a good chance of detecting the effects that they want to do. And so this is required just like a range of kinds of approaches to different statistical models and figuring out how people think about. Their statistical analyses and their experiments.

[00:07:10] But it's been definitely a very cool project. 

[00:07:15] **Nicole:** Sure. So this is a computationally. Intensive project. Was this your first experience doing computational work? 

[00:07:25] **Elizabeth:** This was my first substantial experience I had, used, R and M plus and stuff. So. Sort of coding, you know, for isolated analyses, but I had not really done very much in the way of kind of a formal software development. So it was a very new experience. And I started on with the, the PIs had just been granted the and in terms of coding the actual software, I was the only person doing that.

[00:07:57] So I kind of started. Alone doing that and testing it and trying to set up some infrastructure at a time where I had, probably a mere months prior learned what version control was, you know, so certainly challenging. And particularly as we started adding more people on the project and kind of learning how work in the context of software development team and interface with the UI UX people and, you know, working on the GUI. I had to learn quickly. 

[00:08:29] **Nicole:** Yeah, absolutely. You self-taught a lot of languages and technologies. And if I can say, so you do a really good job advocating and teaching yourself best practice.

[00:08:43] I'm wondering if you have any lessons learned or any advice for grad students that might be in similar positions? I think it's a pretty common occurrence, right? For, for grad student to sort of be in charge of all this stuff. 

[00:08:59] **Elizabeth:** Yeah. It's definitely been an kind of trial by fire. If I could go back I definitely would do things a lot more cleanly. But I think that sometimes you're just stressed into a project and particularly if you're the one trying to spearhead it and teach yourself a bunch of things. I mean, I'll give advice with the caveat of, this is the way that I did it and it worked out okay.

[00:09:23] I guess maybe like one pathway there may be others, but in terms of my perspective, I think that what helped me most was flexibility. So, just because I had done things a certain way and put a lot of effort into that. If it was, causing problems or coming up short or was not flexible to new needs being able to say, okay, yeah, I worked hard on that and that was working for a while, but now we have new needs and not being hesitant to kind of switch streams.

[00:09:55] I mean maybe two or three years into the project, I switched basically everything from R to Python. And, you know, that was, that was a, big switch. But it definitely, I think, led to us being able to accomplish a lot more things. In terms of branching out into new modules, adding new features to the product. And I mean, I don't know whether you agree or not, but I think, I think it made it a little bit easier to hook up to the user interface.

[00:10:26] And you know, just my version control systems, you know, just being flexible to noticing when things are getting messy, when people are having difficulty like staying on board and Yeah, that that was the thing. And then the other thing is not getting scared off. I think a lot of people when they are thrust into a project, whether it's in a management role or as you know a member of the team, not in a management role, People get, can get overwhelmed, especially if they're early career and it can seem like there is nothing you can do.

[00:11:05] Like it's, it's hard to know where to start. And, you know, I think that if you just plow ahead and break things down and just figure out one thing you don't know, and then get on stack overflow, get on Google, get on, you know, YouTube, however, whatever resources you can find. And if you don't get scared off by the problems that seem overwhelming, you know, as you start to kind of research things and delve into resources and, you know, just try to write code things kind of can unfold a little bit more naturally.

[00:11:39] And I think that a lot of people just get stuck at the phase of this. It's so overwhelming and there's so many things I don't know. So I, I don't even want to get started or I want to, have someone else do it, or I, will just kind of go to someone else and say, I'm having this problem, which is also good.

[00:12:00] It's good to reach out and collaborate with people. But, you know, I think that being willing to just open that tab, go to Google and just start exploring is, is a major step. 

[00:12:13] Yeah, I think it is definitely a skill to learn to just try it and get information from trial and error just by planning or researching.

[00:12:26] I think that's a really great point. You brought up that you were doing a lot of training and documentation. You were doing a lot of things in a managerial position. So what do you feel like that you've learned about onboarding students and particularly people with non-technical backgrounds?

[00:12:54] Yeah, I think that one thing I learned is that I'm good at training. I'm going to teaching I'm good at documentation and I'm not a great manager. You know, I, the day-to-day stuff is a little bit more difficult for me, keeping on top of, early on handholding and really making sure they understand what's being communicated, but hopefully I'm getting better.

[00:13:19] I think that. It's very important to orient people to where resources are. So if you have, you know, training documents or like orientation materials and things like that consolidating that as much as possible is very helpful. Because a lot of people get tripped up because they don't know something and they know that there are resources that have been developed for training, but, you know, they don't know where.

[00:13:45] And so having a document of this is where this is, this is where that is. And having a few of those kinds of repositories of information as possible is very helpful. And I found ways actually to integrate these kinds of materials into the repository itself. So, into the discussions tabs, depending on what version of get hub you're using an issues tab and having step by step.

[00:14:07] Instructions on how to do things, how to test code and. Basically what the point of testing is that that can be helpful because then things are all in one place and people can discuss and ask questions among themselves and posts like, here's this weird issue that I had on my machine. And, here was how I solved it.

[00:14:28] And then some person will randomly have that issue because you have like different drivers or different, randomly. Silly packages one version too old or something, but that's mainly what I've learned so much. It's been kind of, kind of a trip.

[00:14:48] **Nicole:** Yeah. Well, I feel like this role that you had had quite a bit of. Responsibility. And I wonder, what steps that you have taken to hopefully mitigate some of the inevitable burdens that the transfer of power in a big long-term project, like this.

[00:15:13] **Elizabeth:** Documentation documentation is just so important and creating documentation with a mind that someone else is going to be reading it. So you don't want to assume any information. Right. And so I have. Very long docstrings for most of the major classes and functions and vignettes explain exactly how to use of each feature with sort of narrative explanations where appropriate.

[00:15:43] And I think that hopefully that's going to be helpful for people. I particularly on a project where we're going to be recruiting. A lot of people don't have a ton of experience in software development because we tend to recruit research scientists who know about methodology and statistical analysis and experimental design.

[00:16:01] And, you know, they have some coding skills, but maybe not used to a project of this scale. So with that in mind, I would say probably a good 70% of my lines of code are common. With very explicit, this is what this stuff. And definitely also training people on how to use the debugger not just for debugging, but for understanding code, you know, basically opening the hood and seeing line by line exactly how the objects are manipulated.

[00:16:29] Exactly. What's going on. You can really help them understand the logic. 

[00:16:36] **Nicole:** Yeah. All that's really great. And I know part of the, this training you were doing is getting people hooked up to the campus clusters. I want to know how HPC was related to this project. 

[00:16:52] **Elizabeth:** Hm. So a lot of what we're doing is simulation-based, so we're doing the same thing over and over and over again.

[00:17:00] And a lot of these things are very computationally intensive, right. You know, when a researcher is doing it, they only have to run that statistical analysis once and they already have the data. We have to simulate the data and analyze it. And even if say that analysis would take two minutes for the research.

[00:17:20] You do that a thousand times. And you know, you had quite a few hours of work and, you know, eventually we started to try to implement parallel processing and things like that, but that can also get tricky depending on the packages that you have to use for analysis. In addition to that we had to run a lot of different tests because.

[00:17:45] I think unlike a lot of software development projects where you look at it and if it runs without error, you know, it works, you know, more or less, you know, if you can just see that it works. For us, we need to also verify that, it's working exactly in the way that we intend under the hood and that the math is right.

[00:18:05] And sometimes it's difficult to establish very clear external metrics by which we can judge our output. And so, I developed testing systems that would basically go through all the combinatorics every way the user could specify something and try to compare those to whatever either a Priore or empirical metrics that I could develop to make sure that we're actually simulating what we wanted to simulate, that we were analyzing it properly. And this requires a lot of tests, a lot of tests with a lot of both numeric and combinatoric coverage. And so it got to the point where this is just not something that could be run on a local machine.

[00:18:49] And last we wanted to wait weeks and weeks and weeks. So we got the supercomputing cluster and that's also been quite a trip learning how to do that, but it's definitely sped up the process substantially. 

[00:19:01] **Nicole:** Yeah. So I think this is a really common problem. Having to validate computational code, make sure that, you know, you're numerically getting something reasonable.

[00:19:13] So what is it that you're looking for or that you're trying to protect against when you do this kind of testing? 

[00:19:21] **Elizabeth:** Mainly me doing misunderstanding the math, underlying the statistics, you know, it's like, oh, well, I thought that was a simple matrix multiplication, but actually that's a chronic or product, you know, or or just that I, in some line of code one bug I found recently is I.

[00:19:40] Forgot to add one index for a data frame that I didn't add. So, it ran without error and a lot of the numbers look reasonable. But some of them did it and it was because I was not adding that column of the matrix into that, into that multiplication. And so, you know, such a simple thing, if you characters and you know, it, completely changes the way that things are calculated. And so mainly I'm trying to protect against my own inexperience and, or confusion. 

[00:20:13] **Nicole:** Sure. And I don't even think it's necessarily experience that prevents us from making bugs. I think that's just a, sort of a certainty that comes along shore coding.

[00:20:25] So I think that's a Valiant effort often not taken

[00:20:32] . I appreciate 

[00:20:33] **Elizabeth:** that. For me, a lot of the statistical concepts that we're trying to implement, in fact, I would say possibly even a majority of them. I didn't really know when we started. And so I, that was another thing that was just hours of Googling and reading papers and trying to make sure that I understood how these statistical analysis actually worked and , when you teach yourself something, a lot of the times it happens imperfectly and so that, that definitely has been quite challenging. 

[00:21:11] **Nicole:** Yeah. All that's really great. So now Elizabeth, I would like to know about your own research. You did, you just defended. So why don't you tell us what you were working on? 

[00:21:22] Sure. So essentially when I got back a little bit to my cognitive neuroscience roots and applied it to clinical psychology and what I was trying to figure out if there are ways that we can use mathematical modeling to identify people who have schizophrenia And the data that I had, it's an archival data set.

[00:21:43] Been in the works since I was maybe three years old. So I don't, I don't claim this marvelous data collection, but, I was lucky enough to inherit it. And it was a simple cognitive task that involved basically speed accuracy, trade offs.

[00:21:57] So, you try to go as quickly as you can while still being accurate on. And we know that people with schizophrenia tend to have more difficulty with this task. They also tend to have more difficulty recognizing and adjusting when they make a mistake. And so for most neuro-typical people on this very simple task they might answer a little bit too quickly and say, oh shoot, you know, that, that wasn't right.

[00:22:23] And they will slow down. And I was interested in modeling the within person process. So they do multiple trials of this task and I wanted to see, can I characterize using just a mathematical formula an individual person's process of converging on that optimal response time and responding and altering that process if they've made a mistake. And so I applied a variety of mathematical models, including some piecewise linear and exponential models and some damped oscillator models and just ran them through some machine learning classification procedures to see if I could, use those model parameters that I fit to each person to identify people with a schizophrenia diagnosis.

[00:23:09] And you know, my best models ended up with 81% accuracy, which, in this area is pretty good. So I think that pending replication and other samples and some refinement, this could be something that is promising for clinical implementation. 

[00:23:25] Excellent. Yeah. That's that's interesting stuff. What is your big next step now that you've you have all this research and computing experience under your belt?

[00:23:37] What what's next for you? 

[00:23:39] **Elizabeth:** Oh, boy, that's a big question. So I'm, still kind of figuring out my next steps, but I have a potential kind of soft offer for post-doc position at the Yale school of medicine. Funny, funnily enough, I'm ending up in the same exact building where I conducted my undergrad cognitive neuroscience research 11 years ago.

[00:24:03] And so yeah, it's, it would be, it would be very interesting. And there are two different startups that I'm somewhat involved in. One of which I co-founded the other, which I'm just in this past week have started getting involved with talking with the founder who coincidentally enough.

[00:24:20] She's a friend from college is working with one of the former mentors of my potential post-doc advisor. So small world, small world. But essentially what I want to do research. And there are many avenues for that. Postdocs, there are military labs, there are national labs. I expect that one day I'll end up in national lab or military lab at some point in my career.

[00:24:43] Yeah. 

[00:24:44] **Nicole:** Yeah. And so not that you have to know now, but when you're considering all these career paths, What are the most important factors for you right now that are in the 

[00:24:56] **Elizabeth:** balance? Yeah, so I want to be able to do interesting research that will have high impact.

[00:25:05] I want academic freedom, you know, I want to be able to, choose my own projects. I've been. Uncommonly fortunate. And my grad school career to have an advisor who lets me do whatever I want. So I definitely don't want to, now that I have my PhD downgrade and have less freedom than I had as a first year grad student, it's a little hard to go back right. And so, you know, definitely, you know, while some industry positions are interesting , I'm not going to be able to choose my own projects. And so choosing my own projects, I think is probably the biggest consideration. 

[00:25:43] **Nicole:** I definitely understand the draw of intellectual freedom.

[00:25:50] So what is it now that you've joined USR? What is it that you hope to get out of your membership to that community? 

[00:25:59] **Elizabeth:** You know, I think that we increasing collaboration between more kind of theoretical and basic researchers and actual software engineers is super important because a lot of projects in academia get published and they say, future directions, here are some applications, no one ever applies because it isn't rewarded. And the training and how to implement these things is uncommon among researchers. And so I think that, you know, increasing communication between people in university contexts and people who actually know how to create the infrastructure to support the dissemination of our scientific findings is really the next step in actually making fields like psychology and neuroscience have a broader impact in one, in an impact that's more immediately. Rather than just sort of trickling down across decades. 

[00:27:01] Yeah. I think very eloquently put, I have nothing to add. So I want to thank you very much for, for coming on. And also I'm looking forward to hearing where you end up. I'm sure you'll have just. as interesting and non-linear of a career as where you started with your education and then I'm sure it will also be very interesting.

[00:27:31] Oh, I, I certainly hope so. 

[00:27:34] **Nicole:** So thank you very much.

[00:27:36] **Elizabeth:** And thank you for having me. 

[00:27:39] 

